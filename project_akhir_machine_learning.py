# -*- coding: utf-8 -*-
"""Project-Akhir-Machine-Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19z7K_U_kaoTnp5VPvbwOJjCJZWUUBHXv

# **Problem dan Goals**

Berdasarkan analisis data Global Economy Indicators, berikut insights utama:

Problems:
1. Kesenjangan signifikan antar negara dalam indikator ekonomi
2. Korelasi kuat antara tata kelola (governance) dengan kinerja ekonomi
3. Beberapa negara memiliki skor sangat rendah di keamanan dan kebebasan personal

Goals:
1. Mengelompokkan negara berdasarkan karakteristik ekonomi (menggunakan K-means clustering)
2. Mengidentifikasi faktor-faktor yang paling berpengaruh terhadap skor rata-rata ekonomi
3. Menemukan pola dan hubungan antar indikator untuk pembuatan kebijakan

Dari analisis clustering:
- Optimal cluster = 3 (berdasarkan elbow method dan silhouette score)
- Negara dapat dikelompokkan menjadi: ekonomi maju, berkembang, dan tertinggal

Temuan ini dapat membantu pembuat kebijakan untuk:
- Mengidentifikasi area prioritas perbaikan
- Merancang intervensi berdasarkan karakteristik cluster
- Menetapkan target pembangunan yang realistis

# **Dataset**
Jumlah Baris: 167

Jumlah Kolom: 14

Numerical Features (Fitur Numerik):
* AveragScore,
* SafetySecurity,
* PersonelFreedom,
* Governance, SocialCapital,
* InvestmentEnvironment,
* EnterpriseConditions,
* MarketAccessInfrastructure,
* EconomicQuality,
* LivingConditions,
* Health,
* Education,
* NaturalEnvironment

Categorical Features (Fitur Kategorikal):

*  Country

# **Codingan**s
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
import pickle
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt

dataf = pd.read_csv('data.csv')
dataf.head()

features = ['SafetySecurity', 'PersonelFreedom', 'Governance',
           'SocialCapital', 'InvestmentEnvironment', 'EnterpriseConditions',
           'MarketAccessInfrastructure', 'EconomicQuality', 'LivingConditions',
           'Health', 'Education', 'NaturalEnvironment']
df = dataf[features]

df.describe()

df.info()

df.isnull().sum()

import seaborn as sns

# Set figure size
plt.figure(figsize=(15, 40))

# Create subplots for each column
for idx, column in enumerate(df.columns[1:], 1): # Skip Country column
   plt.subplot(7, 2, idx)
   sns.histplot(data=df, x=column, bins=30, color='darkblue', alpha=0.7)
   plt.title(f'Distribution of {column}')
   plt.xlabel(column)
   plt.ylabel('Count')
   plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Create subplot for all numerical columns
plt.figure(figsize=(15, 40))

# Plot boxplots
for idx, column in enumerate(df.columns[1:], 1): # Skip Country column
   plt.subplot(7, 2, idx)
   sns.boxplot(data=df, y=column, color='darkblue')
   plt.title(f'Distribution of {column}')
   plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 40))

# Loop melalui setiap fitur numerik
for idx, column in enumerate(df.columns[1:], 1):  # Lewati kolom 'Country'
    # Buat subplot untuk setiap fitur
    plt.subplot(7, 2, idx)

    # Hitung batas atas dan bawah untuk mendeteksi outlier
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter data untuk menghapus outlier
    filtered_data = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

    # Buat box plot tanpa outlier
    sns.boxplot(data=filtered_data, y=column, color='darkblue', showfliers=False)
    plt.title(f'Distribusi {column} (Tanpa Outlier)')
    plt.xticks(rotation=45)

# Atur layout dan tampilkan plot
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Create correlation matrix
plt.figure(figsize=(15, 10))
sns.heatmap(df.iloc[:, 1:].corr(),
           annot=True,
           cmap='coolwarm',
           fmt='.2f',
           square=True,
           annot_kws={'size': 8})

plt.title('Correlation Heatmap of Global Economy Indicators')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


# Prepare data
X = df.select_dtypes(include=[np.number]) # Select numeric columns

# Function to calculate inertia for different k values
def calculate_inertia(data, k_range):
   inertias = []
   silhouette_scores = []

   for k in k_range:
       kmeans = KMeans(n_clusters=k, random_state=42)
       kmeans.fit(data)
       inertias.append(kmeans.inertia_)
       if k > 1:
           silhouette_scores.append(silhouette_score(data, kmeans.labels_))

   return inertias, silhouette_scores

# Plot elbow curves for different scalers
scalers = {
   'No Scaling': None,
   'Standard Scaler': StandardScaler(),
   'MinMax Scaler': MinMaxScaler(),
   'Robust Scaler': RobustScaler()
}

plt.figure(figsize=(15, 10))

for idx, (scaler_name, scaler) in enumerate(scalers.items(), 1):
   # Scale data if scaler exists
   X_scaled = scaler.fit_transform(X) if scaler else X

   # Calculate inertia and silhouette scores
   k_range = range(1, 11)
   inertias, silhouette_scores = calculate_inertia(X_scaled, k_range)

   # Plot
   plt.subplot(2, 2, idx)
   plt.plot(k_range, inertias, 'bo-')
   plt.title(f'Elbow Method with {scaler_name}')
   plt.xlabel('Number of Clusters (k)')
   plt.ylabel('Inertia')
   plt.grid(True)

plt.tight_layout()
X_scaled = StandardScaler().fit_transform(X)

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X_scaled)

principalDf = pd.DataFrame(data = principalComponents,
                          columns = ['principal component 1', 'principal component 2'])
plt.show()

X_scaled = StandardScaler().fit_transform(X)

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X_scaled)

principalDf = pd.DataFrame(data = principalComponents,
                          columns = ['principal component 1', 'principal component 2'])


from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# 5. Add cluster labels to the principal components DataFrame
finalDf = pd.concat([principalDf, pd.Series(cluster_labels, name='Cluster')], axis=1)

# 6. Visualize the results
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(1, 1, 1)
ax.set_xlabel('Principal Component 1', fontsize=15)
ax.set_ylabel('Principal Component 2', fontsize=15)
ax.set_title('2 Component PCA with Clusters', fontsize=20)

# Define targets and colors for each cluster
targets = [0, 1, 2]  # Cluster labels
colors = ['r', 'g', 'b']  # Colors for each cluster

for target, color in zip(targets, colors):
    indicesToKeep = finalDf['Cluster'] == target
    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1'],
              finalDf.loc[indicesToKeep, 'principal component 2'],
              c=color,
              s=50,
              label=f'Cluster {target}')

ax.legend()
ax.grid()
explained_var = pca.explained_variance_ratio_
plt.xlabel(f'Principal Component 1 ({explained_var[0]:.2%} explained variance)')
plt.ylabel(f'Principal Component 2 ({explained_var[1]:.2%} explained variance)')

plt.grid(True)
plt.show()

"""PC1 mungkin merepresentasikan "Kekuatan Ekonomi & Infrastruktur" karena memiliki bobot tinggi pada indikator ekonomi

PC2 mungkin merepresentasikan "Kualitas Hidup & Sosial" karena berkaitan dengan kesehatan, pendidikan, dan kebebasan personal

Cluster 0 (Merah): Terletak di tengah grafik, menunjukkan negara-negara dengan karakteristik ekonomi menengah

Cluster 1 (Hijau): Berada di sebelah kiri grafik, kemungkinan merepresentasikan negara-negara dengan tantangan ekonomi yang lebih besar

Cluster 2 (Biru): Terletak di sebelah kanan grafik, mungkin menunjukkan negara-negara dengan ekonomi yang lebih majupengembangan
"""

feature_weights = pd.DataFrame(
    pca.components_.T,
    columns=['PC1', 'PC2'],
    index=features
)
print("\nKontribusi Fitur to principal components:")
print(feature_weights)

# Calculate total explained variance
total_var = sum(explained_var)
print(f"\nTotal explained variance: {total_var:.2%}")

"""Meskipun tampak kontradiktif, negara dengan tantangan ekonomi besar mungkin memiliki skor PC2 yang tinggi karena beberapa faktor, termasuk kombinasi skor pada berbagai fitur, variasi internal cluster, pengaruh scaling, kompleksitas data, dan interpretasi PC2 yang lebih luas. Penting untuk diingat bahwa PCA dan clustering adalah alat penyederhanaan data, dan interpretasi hasil harus dilakukan dengan hati-hati dan mempertimbangkan konteks data yang dianalisis."""

optimal_k = 3  # Example, replace with your determined value
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans.fit(X_scaled)

!pip install --upgrade streamlit

import pickle

# Save the model to a file
filename = 'kmeans_model.pkl'
pickle.dump(kmeans, open(filename, 'wb'))

import streamlit as st
import pickle
from sklearn.preprocessing import StandardScaler


all_features = ['SafetySecurity', 'PersonelFreedom', 'Governance',
           'SocialCapital', 'InvestmentEnvironment', 'EnterpriseConditions',
           'MarketAccessInfrastructure', 'EconomicQuality', 'LivingConditions',
           'Health', 'Education', 'NaturalEnvironment']

important_features = ['SafetySecurity', 'Governance', 'EconomicQuality', 'LivingConditions']

# Load the model
kmeans_model = pickle.load(open('kmeans_model.pkl', 'rb'))

input_data = {feature: 0.0 for feature in all_features}

st.title('Cluster Prediction')
st.title('Cluster Prediction')
for feature in all_features:
    input_data[feature] = st.number_input(feature, min_value=0.0, max_value=10.0, step=0.1, value=input_data[feature])

# Create input data as a DataFrame
input_df = pd.DataFrame([input_data])

# Scale Preprocessing Input Data
scaler = StandardScaler()
scaled_input = scaler.fit_transform(input_df[all_features])

# Predict cluster
predicted_cluster = kmeans_model.predict(scaled_input)[0]

# Display the result
st.write(f'Predicted Cluster: {predicted_cluster}')